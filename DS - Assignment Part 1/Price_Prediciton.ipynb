{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Libraries"
      ],
      "metadata": {
        "id": "eJN7VxOBBI_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6K4vLfYCutU"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics                  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data using excel format\n",
        "data = pd.read_excel(\"/content/DS - Assignment Part 1 data set.xlsx\")"
      ],
      "metadata": {
        "id": "rwbTAw8eDdwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unmistakable Insights of the Dataset"
      ],
      "metadata": {
        "id": "jh32SzM8Fl4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "PEu9ziN9Dimz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info"
      ],
      "metadata": {
        "id": "HDxcKlMbD-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull()"
      ],
      "metadata": {
        "id": "LCakRxUqEBOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection\n",
        "##### We will involve channel technique for highlight determination. In this strategy, sifting is finished utilizing relationship network and it is most regularly done utilizing corrleation and VIF."
      ],
      "metadata": {
        "id": "cHUR9_dxFPZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sb.pairplot(data,diag_kind=\"kde\")"
      ],
      "metadata": {
        "id": "bpFjcCVbYmsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Values."
      ],
      "metadata": {
        "id": "4ntJ80jbFEMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definig the function to find the missing Values\n",
        "df_miss = pd.DataFrame(100*data.isnull().sum()/data.shape[0]).reset_index()"
      ],
      "metadata": {
        "id": "eif7YKH6ZU1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_miss"
      ],
      "metadata": {
        "id": "Lwc7-oagaOnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Characterizing Data sources and the Objectives "
      ],
      "metadata": {
        "id": "um44RpzZE2ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=data.drop('House price of unit area', axis=1)\n",
        "y= data['House price of unit area']"
      ],
      "metadata": {
        "id": "IZ8lAqsroocG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=365)"
      ],
      "metadata": {
        "id": "EaAkKlh2DwFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "reg = LinearRegression()\n",
        "reg.fit(x_train,y_train)\n",
        "y_hat = reg.predict(x_train)"
      ],
      "metadata": {
        "id": "3oH7GrzSDwHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg.score(x_train,y_train).round(3)"
      ],
      "metadata": {
        "id": "Z7DMFZDrDwKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "58.10% of the information fit the relapse model."
      ],
      "metadata": {
        "id": "ORl_fbTtEkFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_resid = y_train - y_hat\n",
        "\n",
        "fig, ax = plt.subplots(1,2, constrained_layout = True)\n",
        "    \n",
        "sb.regplot(x=y_hat, y=y_train, ax=ax[0], line_kws={'color': 'red'})\n",
        "ax[0].set_title('Observed vs. Predicted Values', fontsize=10)\n",
        "ax[0].set(xlabel='Predicted', ylabel='Observed')\n",
        "\n",
        "sb.regplot(x=y_hat, y=y_resid, ax=ax[1], line_kws={'color': 'red'})\n",
        "ax[1].set_title('Residuals vs. Predicted Values', fontsize=10)\n",
        "ax[1].set(xlabel='Predicted', ylabel='Residuals')"
      ],
      "metadata": {
        "id": "IG0kJkzgDwLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The focuses are not evenly conveyed around the slanting line neither around the level line. Thus, there is infringement of the linearity and homogeneity presumption."
      ],
      "metadata": {
        "id": "VYGfoQvxEUYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_resid.hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nvYYkt7dDwR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The residuals are slanted on the right or we can say the circulation of residuals is non-typical. Thus, it further abuses the OLS suspicions\n",
        "\n",
        "Disregarding these infringement, I will test the model."
      ],
      "metadata": {
        "id": "9I1zYUFAEcY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the Weights and Bias"
      ],
      "metadata": {
        "id": "bZsyN1d3Ezzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intercept = reg.intercept_.round(3)"
      ],
      "metadata": {
        "id": "GwJTX78BDwUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coeff = reg.coef_.round(3)\n",
        "coeff"
      ],
      "metadata": {
        "id": "FBaLpiACDwXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "wNzT4uxDGPMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_test = reg.predict(x_test)"
      ],
      "metadata": {
        "id": "WJehSjxYDwch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg.score(x_test,y_test).round(3)"
      ],
      "metadata": {
        "id": "XTsfe-M-Dwea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_resid_test = y_test - y_hat_test\n",
        "\n",
        "fig, ax = plt.subplots(1,2, constrained_layout = True)\n",
        "    \n",
        "sb.regplot(x=y_hat_test, y=y_test, ax=ax[0], line_kws={'color': 'red'})\n",
        "ax[0].set_title('Observed vs. Predicted Values', fontsize=10)\n",
        "ax[0].set(xlabel='Predicted', ylabel='Observed')\n",
        "\n",
        "sb.regplot(x=y_hat_test, y=y_resid_test, ax=ax[1], line_kws={'color': 'red'})\n",
        "ax[1].set_title('Residuals vs. Predicted Values', fontsize=10)\n",
        "ax[1].set(xlabel='Predicted', ylabel='Residuals')"
      ],
      "metadata": {
        "id": "QOd8fhDZDwhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our straight model isn't proper as it disregards the OLS suppositions of Ordinariness and homogeneity. There are ways of fixing these issues which requires definite concentrate on my part as I'm a fledgling in ML. Up until this point I delighted in playing out this examination, and my comprehension on relapse procedure has expanded. You advance by doing."
      ],
      "metadata": {
        "id": "23cjHU3XGKFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Regression"
      ],
      "metadata": {
        "id": "LfTeMJy4LOEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An irregular woods is a meta assessor that fits various grouping choice trees on different sub-tests of the dataset and utilizes averaging to work on the prescient exactness and command over-fitting."
      ],
      "metadata": {
        "id": "U3ZS9DeQLdNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "JtK-qVZEGTyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate a fresh instance of RandomForestRegressor"
      ],
      "metadata": {
        "id": "VZDL9pO4PqsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_pipeline = Pipeline([('std_scaler', StandardScaler())])\n",
        "prepared_data = num_pipeline.fit_transform(X)"
      ],
      "metadata": {
        "id": "lwHRBqZCGhNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(prepared_data, y)"
      ],
      "metadata": {
        "id": "uo14Gl5NG4oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(prepared_data, y)"
      ],
      "metadata": {
        "id": "LZrcxKLIG--n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can evaluate the loaded model to see if its predictions line up with the predictions made prior to saving.Disregarding these infringement, I will test the model."
      ],
      "metadata": {
        "id": "j52Su7daPu3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_predictions = forest_reg.predict(prepared_data)\n",
        "forest_mse = mean_squared_error(y, dataset_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "forest_rmse"
      ],
      "metadata": {
        "id": "pdJqNi23G_1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction computed with out-of-bag estimate on the training set."
      ],
      "metadata": {
        "id": "ONOo-XXpP7Zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_scores(scores):\n",
        "    print('Scores:', scores)\n",
        "    print('Mean:', scores.mean())\n",
        "    print('Standard deviation:', scores.std())"
      ],
      "metadata": {
        "id": "GIgP-8A6HGHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_scores = cross_val_score(forest_reg, prepared_data, y,\n",
        "                            scoring='neg_mean_squared_error', cv=10)\n",
        "forest_rmse_scores = np.sqrt(-forest_scores)\n",
        "display_scores(forest_rmse_scores)"
      ],
      "metadata": {
        "id": "q5Gp0-ydHHJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This mean squared error result is lower than our base model which is great to see but overall, I’d still consider this performance inadequate. A root mean  of 7.369 means that the average error per estimate."
      ],
      "metadata": {
        "id": "DClGqMdkAq5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Neural Networks"
      ],
      "metadata": {
        "id": "rLe8TDGsKQdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=X.to_numpy()\n",
        "s_input=StandardScaler()\n",
        "inputs=s_input.fit_transform(inputs)"
      ],
      "metadata": {
        "id": "MvuEmnF7H7RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "benchmark base network model for the relapse issue. Beginning with the required capabilities in general and items."
      ],
      "metadata": {
        "id": "tLvAk3BRD-P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target=y.to_numpy()"
      ],
      "metadata": {
        "id": "rAxVoGr-KWrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('tf_data',inputs=inputs,targets=target)"
      ],
      "metadata": {
        "id": "VFpZx_2lKgSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datatf=np.load('tf_data.npz')\n",
        "inputss=datatf['inputs']\n",
        "targets=datatf['targets']\n",
        "print(targets.shape)\n",
        "output_size=1\n",
        "input_size=inputss.shape[1]\n",
        "print(input_size)"
      ],
      "metadata": {
        "id": "vGEk4w6sKhWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model=tf.keras.Sequential([tf.keras.layers.Dense(8,input_shape=(8,),activation='relu'),tf.keras.layers.Dense(8,activation='relu'),tf.keras.layers.Dense(output_size)])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "dsaZCyQdKjKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 8 highlights, we should embed 8 neurons as a beginning, 4 secret layers, and 1 result layer due to anticipated house costs.\n",
        "\n",
        "Likewise, ADAM streamlining calculation is utilized for upgrading misfortune capability (Mean squared blunder"
      ],
      "metadata": {
        "id": "waKaF3hrEVpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(inputss,targets,epochs=1500,verbose=1)\n",
        "weights=model.layers[0].get_weights()[0]\n",
        "bias=model.layers[0].get_weights()[1]"
      ],
      "metadata": {
        "id": "HRepvki1K1il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, train the model for auto epochs, recording the preparation and approval exactness in the set of experiences each time. The model will run both training and testing information while working out the misfortune capability to monitor how well the model performs for each age."
      ],
      "metadata": {
        "id": "nQUgqdL1DZp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=model.predict(inputss)"
      ],
      "metadata": {
        "id": "gFlFJdzfK3fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "id": "iLklzm0KLIWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shock, this score can be further developed through highlight determination or utilizing other relapse models."
      ],
      "metadata": {
        "id": "ZUPLnTYhDlEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame (prediction) # converting in to dataframe of all predicitons."
      ],
      "metadata": {
        "id": "T4YpUA1NMAE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv = pd.read_excel(\"/content/DS - Assignment Part 1 data set.xlsx\") # reading the xlsx."
      ],
      "metadata": {
        "id": "3suRVgofCMQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv['prediction']=df #stored in as a new parameter."
      ],
      "metadata": {
        "id": "bwc9zppACwlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv.to_csv('output.csv')"
      ],
      "metadata": {
        "id": "2Kk4zWd-Epr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The focuses are not evenly disseminated around the slanting line neither around the flat line. Subsequently, there is infringement of the linearity and homogeneity presumption."
      ],
      "metadata": {
        "id": "NSdqCl9vPduA"
      }
    }
  ]
}